{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a782ce",
   "metadata": {},
   "source": [
    "# Semantic search for job postings\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1152d7",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "**Use case:**\n",
    "- a user gives a query with his main skills, and gets Linkedin job listings matching these skills. Simple general similarity here might not be the best option, cause queries greatly differ from job posting.\n",
    "\n",
    "**The task:**\n",
    "- Fine-tune an embedding model to optmize semantic job search\n",
    "\n",
    "**Example:**\n",
    "- input: \"data scientis 6 years expierience, LLMs, credit risk, content marketing\" \n",
    "- positive match: job listings for data scientist position\n",
    "- negative match: job listings for data engineer possition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea8bc0",
   "metadata": {},
   "source": [
    "> **Fine-tuning text embeddings for specific user case**\n",
    ">\n",
    "> A bit on embeddings and fine-tuning:\n",
    "> - The purpose is to fine-tune general text embeddings (to improve semantic search) for specific domain or use case.\n",
    "> - Fine-tuning is adapting a model to specific use case with additonal training.\n",
    "> - Text embedding - semantically meaningful vector.\n",
    "> - Using base model embeddings, we retrieve most similar chunks, but the most similar chunk is not always what we are looking for.\n",
    "> - Fine-tuned model can be further fine-tuned to add specific behaviour.\n",
    ">\n",
    "> Method:\n",
    "> - **Contrastive Learning** (CL) in embeddings is a machine learning technique used to train models to create a representation space (embedding space) where similar data points are mapped close to each other, and dissimilar data points are pushed farther apart.\n",
    "> - The core idea of contrastive learning revolves around the construction and use of sample pairs:\n",
    ">     - Anchor Sample (x): The original data point.\n",
    ">     - Positive Pair (x+): A data point that is semantically similar or related to the anchor.\n",
    ">     - Negative Pairs (x−): Data points that are semantically dissimilar or unrelated to the anchor.\n",
    "> - The training objective: the model (an encoder network) is trained using a contrastive loss function (like InfoNCE or Triplet Loss) that optimizes the distances between the embeddings of these pairs:\n",
    ">     - Pull Positives Closer: The loss function penalizes the model when the embedding distance between the Anchor and the Positive Pair is large (or their similarity is low). This forces similar data into tight clusters in the embedding space.\n",
    ">     - Push Negatives Apart: The loss function penalizes the model when the embedding distance between the Anchor and a Negative Pair is small (or their similarity is high). This separates dissimilar data, ensuring the embeddings are highly discriminative.\n",
    ">\n",
    "> By minimizing this loss, the model learns meaningful vector representations (embeddings) that effectively capture the inherent similarity and dissimilarity relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea54a27",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775faa4",
   "metadata": {},
   "source": [
    "**Dataset construction:**\n",
    "1. Download job listings dataset: https://huggingface.co/datasets/datastax/linkedin_job_listings\n",
    "2. Keep only: [\"Data Scientist\", \"Data Analyst\", \"Machine Learning Engineer\", \"Data Engineer\", \"AI Engineer\", \"Deep Learning\"]\n",
    "3. Keep job description field (job_description_pos)\n",
    "4. Create a dataset with queries generated from job descriptions (query)\n",
    "5. Create negative samples pairs by matching least similar queries and job descriptions (job_description_neg)\n",
    "6. Shuffle and split: train: 0.8, test: 0.1, validation: 0.1\n",
    "7. Upload to huggingface: dataset_dict.push_to_hub(\"shawhin/ai-job-embedding-finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a7d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'job_description_pos', 'job_description_neg'],\n",
      "        num_rows: 809\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'job_description_pos', 'job_description_neg'],\n",
      "        num_rows: 101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'job_description_pos', 'job_description_neg'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n",
      "**************************************************\n",
      "{'query': 'Data engineering Azure cloud Apache Spark Kafka', 'job_description_pos': 'Skills:Proven experience in data engineering and workflow development.Strong knowledge of Azure cloud services.Proficiency in Apache Spark and Apache Kafka.Excellent programming skills in Python/Java.Hands-on experience with Azure Synapse, DataBricks, and Azure Data Factory.\\nNice To Have Skills:Experience with BI Tools such as Tableau or Power BI.Familiarity with Terraform for infrastructure as code.Knowledge of Git Actions for CI/CD pipelines.Understanding of database design and architecting principles.Strong communication skills and ability to manage technical projects effectively.', 'job_description_neg': 'requirements, and assist in data structure implementation planning for innovative data visualization, predictive modeling, and advanced analytics solutions.* Unfortunately, we cannot accommodate Visa Sponsorship for this role at this time.\\nESSENTIAL JOB FUNCTIONS\\nMine data covering a wide range of information from customer profile to transaction details to solve risk problems that involve classification, clustering, pattern analysis, sampling and simulations.Apply strong data science expertise and systems analysis methodology to help guide solution analysis, working closely with both business and technical teams, with consideration of both technical and non-technical implications and trade-offs.Carry out independent research and innovation in new content, ML, and technological domains. Trouble shooting any data, system and flow challenges while maintaining clearly defined strategy execution.Extract data from various data sources; perform exploratory data analysis, cleanse, transform, and aggregate data.Collaborate with New Product Strategy, Decision Science, Technology Development, Business Intelligence, and business leaders to define product requirements, provide analytical support and communicate feedback.Assess the efficiency and accuracy of new data sources and optimize data gathering techniques.Communicate verbally and in writing to business customers with various levels of technical knowledge, educating them about defined solutions, as well as sharing insights and recommendations.\\n\\nCANDIDATE REQUIREMENTS\\nMS in Data Science, Data Engineering, mathematics, Computer Science, Statistics, or related field, or equivalent working experience5+ years of relevant experience in Data Science, Data Analytics, Applied Statistics, or another quantitative field preferred2+ years using R, Python or SQL to manipulate data and draw insights from large data setsExperience working in cloud environments for data science workloadsPrevious experience working within banking and / or other financial services industries a plusStrong creative thinking and problem-solving skillsExcellent oral and written communication and presentation skills\\nWHO WE ARE \\nVALID Systems is comprised of two differentiating ingredients. Our Technology and our Team. VALID’s core capabilities are driven by our fully automated transaction processing and patented risk decision engine, and our team of data scientists, technologists, risk analysts, innovators, and industry leaders bring these capabilities to life for our clients. This enables VALID to offer the most highly customized solutions that execute on the near impossible mission of minimizing risk, enhancing the customer experience, all at a profit for our clients. We are meticulous about our data, relentless in solving problems, and maniacal in the pursuit of our clients’ success. \\nTHE TECHNOLOGY Our technology allows our clients to make the right transactional decisions, in real-time, and drive revenue. Leapfrogging the conventional static set of industry based risk rules and 2 day old account status responses, VALID leverages a proprietary risk modeling architecture that employs predictive analytics. Focusing on the key predictive data attributes and behavioral patterns, each decision, positive pay, and return are fed back into the automated decision engine, thereby creating a self-learning model that remains in a state of perpetual refinement. While the principles of VALID’s risk modeling are increasingly technical, extensive emphasis has been placed on both up front data attribute and decision response flexibility that allows for client specific tailoring. We provide this level of sophistication on each decision not only in sub-second real-time transaction speeds, but with industry leading security within our platform and data management. \\nTHE TEAM Since 2003 VALID has focused on acquiring talent with an expertise that reflects its client base. Equal to that focus has been equipping that talent with the ability to execute against major initiatives and deliver on the objectives of our partners and clients. To that end VALID has fostered a culture that encourages our world-class talent to push the edges of conventional processes and think outside the box when facing problems. We develop solutions not to simply fix a problem, but looking ahead to better an industry. \\nOUR CULTURE Google meets Wall-Street. We are casual in dress, but exceptionally professional in our expectations of our employees. We are all experts in our own business areas. We rely on one another, and trust has to be high for this to be successful. We value accountability in the workplace and family. We may not be monitoring you but we expect you to monitor yourself. \\nIf you ask the people who work here, we’d tell you none of us has ever worked at a company quite like VALID Systems!'}\n",
      "{'query': 'Databricks, Medallion architecture, ETL processes', 'job_description_pos': \"experience with Databricks, PySpark, SQL, Spark clusters, and Jupyter Notebooks.- Expertise in building data lakes using the Medallion architecture and working with delta tables in the delta file format.- Familiarity with CI/CD pipelines and Agile methodologies, ensuring efficient and collaborative development practices.- Strong understanding of ETL processes, data modeling, and data warehousing principles.- Experience with data visualization tools like Power BI is a plus.- Knowledge of cybersecurity data, particularly vulnerability scan data, is preferred.- Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.\\nrequirements and deliver effective solutions aligned with Medallion architecture principles.- Ensure data quality and implement robust data governance standards, leveraging the scalability and efficiency offered by the Medallion architecture.- Design and implement ETL processes, including data cleansing, transformation, and integration, optimizing performance within the delta file format framework.- Build and manage data lakes based on Medallion architecture principles, ensuring scalability, reliability, and adherence to best practices.- Monitor and optimize data pipelines, integrating CI/CD practices to streamline development and deployment processes.- Collaborate with cross-functional team members to implement data analytics projects, utilizing Jupyter Notebooks and other tools to harness the power of the Medallion architecture.- Embrace Agile methodologies throughout the development lifecycle to promote iterative and collaborative development practices, enhancing the effectiveness of Medallion-based solutions.\", 'job_description_neg': \"experience with a minimum of 0+ years of experience in a Computer Science or Data Management related fieldTrack record of implementing software engineering best practices for multiple use cases.Experience of automation of the entire machine learning model lifecycle.Experience with optimization of distributed training of machine learning models.Use of Kubernetes and implementation of machine learning tools in that context.Experience partnering and/or collaborating with teams that have different competences.The role holder will possess a blend of design skills needed for Agile data development projects.Proficiency or passion for learning, in data engineer techniques and testing methodologies and Postgraduate degree in data related field of study will also help. \\n\\n\\nDesirable for the role\\n\\n\\nExperience with DevOps or DataOps concepts, preferably hands-on experience implementing continuous integration or highly automated end-to-end environments.Interest in machine learning will also be advantageous.Experience implementing a microservices architecture.Demonstrate initiative, strong customer orientation, and cross-cultural working.Strong communication and interpersonal skills.Prior significant experience working in Pharmaceutical or Healthcare industry environment.Experience of applying policies, procedures, and guidelines.\\n\\n\\nWhy AstraZeneca?\\n\\nWe follow all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment.\\n\\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That’s why we work, on average, a minimum of three days per week from the office. But that doesn't mean we’re not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\\n\\nCompetitive Salary & Benefits\\n\\nClose date: 10/05/2024\\n\\nSo, what’s next! \\n\\n\\nAre you already imagining yourself joining our team? Good, because we can’t wait to hear from you. Don't delay, apply today!\\n\\n\\nWhere can I find out more?\\n\\nOur Social Media, Follow AstraZeneca on LinkedIn: https://www.linkedin.com/company/1603/\\n\\nInclusion & Diversity: https://careers.astrazeneca.com/inclusion-diversity\\n\\nCareer Site: https://careers.astrazeneca.com/\"}\n",
      "{'query': 'Gas Processing, AI Strategy Development, Plant Optimization', 'job_description_pos': \"experience in AI applications for the Hydrocarbon Processing & Control Industry, specifically, in the Gas Processing and Liquefaction business. Key ResponsibilitiesYou will be required to perform the following:- Lead the development and implementation of AI strategies & roadmaps for optimizing gas operations and business functions- Collaborate with cross-functional teams to identify AI use cases to transform gas operations and business functions (AI Mapping)- Design, develop, and implement AI models and algorithms that solve complex problems- Implement Gen AI use cases to enhance natural gas operations and optimize the Gas business functions- Design and implement AI-enabled plant optimizers for efficiency and reliability- Integrate AI models into existing systems and applications- Troubleshoot and resolve technical issues related to AI models and deployments- Ensure compliance with data privacy and security regulations- Stay up-to-date with the latest advancements in AI and machine learning As a Gas Processing AI Engineer, you will play a crucial role in developing, implementing, and maintaining artificial intelligence solutions that drive business growth and optimized operations. You will collaborate with cross-functional teams to understand business requirements, map new AI trends to address business challenges / opportunities, design AI models, and deploy such models in the gas plants. The ideal candidate should have a strong background in AI and machine learning with hands-on programming and problem-solving skills.  Minimum Requirements\\nAs a successful candidate, you must have a Bachelor's or Master's degree in Chemical Engineering with (10) years of experience in the Oil/Gas industry and significant hands-on experience of AI applications in the Gas Industry.Preferred Qualifications:- PhD or Master's degree in Chemical Engineering- Minimum 10 years of experience in Oil & Gas Industry- Minimum 5 years of Hands-on experience in implementing successful AI projects in the Gas Processing sector- Strong programming skills in Python, TensorFlow, and PyTorch- Experience with reinforcement learning and generative AI (LLM) models- Experience with natural language processing (NLP) and AI Computer Vision- Excellent communication and leadership abilitiesRequirements:- Bachelor's or Master's degree in Chemical Engineering with demonstrated hand-on experience in AI applications and projects- Proven work experience as a Gas Processing AI Engineer or in a similar role- Strong knowledge of machine learning algorithms, neural networks, and deep learning frameworks (e.g., TensorFlow, PyTorch)- Strong knowledge of plant networks and infrastructure requirements to deploy and scale AI in gas plants- Proficiency in programming languages such as Python, Java, or C++- Excellent problem-solving and analytical skills- Strong communication and teamwork abilities- Ability to work on multiple projects and prioritize tasks effectivelyMinimum Years of Experience :09\", 'job_description_neg': \"QualificationsAbility to gather business requirements and translate them into technical solutionsProven experience in developing interactive dashboards and reports using Power BI (3 years minimum)Strong proficiency in SQL and PythonStrong knowledge of DAX (Data Analysis Expressions)Experience working with APIs inside of Power BIExperience with data modeling and data visualization best practicesKnowledge of data warehousing concepts and methodologiesExperience in data analysis and problem-solvingExcellent communication and collaboration skillsBachelor's degree in Computer Science, Information Systems, or a related fieldExperience with cloud platforms such as Azure or AWS is a plus\\nHoursApproximately 15 - 20 hours per week for 3 months with the opportunity to extend the contract further\"}\n",
      "{'query': 'Data Analyst ETL Services, Complex Data Analysis, Refugee Processing Systems', 'job_description_pos': 'requirements, and integrated management systems for our countries civilian agencies (FAA, FDIC, HOR, etc.).Our primary mission is to best serve the needs of our clients by solutioning with our stakeholder teams to ensure that the goals and objectives of our customers are proactively solutioned, such that opportunities to invest our time in developing long-term solutions and assets are abundant and move our clients forward efficiently.At DEVIS, we are enthusiastic about our research, our work and embracing an environment where all are supported in the mission, while maintaining a healthy work-life balance.\\nWe are currently seeking a Data Analyst to join one of our Department of State programs. The candidate would support the Bureau of Population, Refugees, and Migration (PRM) Refugee Processing Center (RPC) in Rosslyn, VA. The ideal candidate must be well-versed in ETL services and adept at gathering business requirements from diverse stakeholders, assessing the pros/cons of ETL tools, and conducting dynamic hands-on evaluation of ETL solutions. The successful candidate will turn data into information, information into insight and insight into business decisions. Data analyst responsibilities include conducting full lifecycle analysis to include requirements, activities and design. Data Analysts will develop analysis and reporting capabilities. They will also monitor performance and quality control plans to identify improvements. ResponsibilitiesInterpret data, analyze results using statistical techniques and provide ongoing reportsDevelop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and qualityAcquire data from primary or secondary data sources and maintain databases/data systemsIdentify, analyze, and interpret trends or patterns in complex data setsFilter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problemsWork with management to prioritize business and information needsLocate and define new process improvement opportunitiesRelated duties or special projects as assigned Required Skills and QualificationsProven working experience as a data analyst or business data analystTechnical expertise regarding data models, database design development, data mining and segmentation techniquesStrong knowledge of and experience with reporting packages (Business Objects etc.), databases (SQL etc.), programming (XML, JavaScript, or ETL frameworks)Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc.)Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracyAdept at queries, report writing and presenting findings Education / CertificationsBachelor’s Degree in Mathematics, Computer Science, Information Management or Statistics Clearance RequirementsMust be a U.S. Citizen with the ability to obtain and maintain a Secret clearance \\nAdditional Perks/BenefitsCompetitive salary compensation 401k Retirement Contribution Savings Plan', 'job_description_neg': \"requirements Skills Required: Have Technical Documentation Skill by translating business requirements into tech specification. Understanding of the GCP ecosystem with a focus on Big Query, DataFlow. Capability of designing and coding analytical solutions for data collections Capability of developing data quality and validation routines Capability of testing data products in development procedure\\n\\nSkills Preferred:\\n\\nStrong Oral and written communication skills o Ability to write complex SQL queries needed to query & analyze data o Ability to communicate complex solution concepts in simple terms o Ability to apply multiple solutions to business problems o Ability to quickly comprehend the functions and capabilities of new technologies.\\n\\nExperience Required:\\n\\n1 years of academic/work experience with one or more of the following: o Data design, data architecture and data modeling (both transactional and analytic) o Building Big Data pipelines for operational and analytical solutions o Running and tuning queries in databases including Big Query, SQL Server, Hive or other equivalent platforms o Data Management - including running queries and compiling data for analytics o Experience with developing code in one or more languages such as Java, Python and SQL\\n\\nExperience Preferred:\\n\\n 2+ year of experience with the following: o GCP Cloud data implementation projects experience (Dataflow, AirFlow, BigQuery, Cloud Storage, Cloud Build, Cloud Run, etc.) Experience with Agile methodologies and tools such as Rally or Jira Certification: Google Professional Data Engineer Experience programming and producing working models or transformations with modern programming languages Knowledge or experience of designing and deploying data processing systems with one or more of the technologies such as Oracle, MS SQL Server, MySQL, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, HBase, Teradata, Tableau, Qlik or Other Strong team player, with the ability to collaborate well with others, to solve problems and actively incorporate input from various sources Demonstrated customer focus, with the ability to evaluate decisions through the eyes of the customer, build strong customer relationships, and create processes with customer viewpoint Strong analytical and problem-solving skills, with the ability to communicate in a clear and succinct manner and effectively evaluates information / data to make decisions Resourceful and quick learner, with the ability to efficiently seek out, learn, and apply new areas of expertise, as needed Highly self-motivated, with the ability to work independently\\n\\nEducation Required:\\n\\nBachelor’s degree in Computer Science, Computer Engineering, Information Technology, or equivalent experience\\n\\nEducation Preferred:\\n\\nMasters degree in Computer Science, Computer Engineering, Information Technology, or equivalent experience are preferred\\n\\nAdditional Information :\\n\\nTech Skill Based Assessment is mandatory. Tech Skill assessment is not fully defined yet how it will be conducted. Hybrid and Remote but Hybrid is preferred\\n\\n\\n\\nApex Systems is \\n\\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\\n\\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\\n\\n4400 Cox Road\\n\\nSuite 200\\n\\nGlen Allen, Virginia 23060\\n\\nApex Systems is\"}\n",
      "{'query': 'Data Engineer SQL Snowflake DBT', 'job_description_pos': 'experience working with relational databases, query authoring (SQL), familiarity with a variety of databases, and DBT (data buid tool) Snowflake Data Warehouse.\\nOverview:** Start date: Immediate** Duration: 2+month W2 contract** Location: Remote from United States, will support core Pacific Time business hours** Compensation: The expected compensation is $54 – 68/hr W2 plus benefits. The offered compensation to a successful candidate will be dependent on several factors that may include (but are not limited to) the type and length of experience within the industry, education, etc.\\nRequirements:\\nBachelor’s degree with 8+ years of experience working on relational databases or Master’s degree with 3 years of experience3-8+ years of experience with SQL and stored procedures, with excellent knowledge in SQL3+ years of experience working on Snowflake, building data warehousing solutions, dealing with slowly changing dimensions as well. 3+ years of experience in developing and deploying data transformations using DBT including creating/debugging macros.5+ experience in supporting end-to-end data model build and maintenance including testing/UAT.Build, maintain and test data pipelines using cloud ETL/ELT tools, preferably Snaplogic.Prior experience in working on SAP HANA.\\nDescription:Develop and maintain scalable data models in Snowflake, ensuring data integrity and reliability.Design and implement data transformations using DBT to support analytics and reporting requirements.Collaborate with data engineers and data analysts to understand data needs and translate them into technical solutions.Optimize Snowflake warehouse configurations and DBT models for performance and cost efficiency.Troubleshoot and resolve data pipeline issues, ensuring smooth and efficient data flow.Participate in code reviews and provide feedback to team members to ensure code quality and adherence to best practices.Stay updated with the latest developments in Snowflake and DBT technologies, and propose and implement innovative solutions.Document data pipelines, transformations, and processes to facilitate knowledge sharing and maintain data lineage.Work closely with cross-functional teams to support data-driven decision-making and business objectives.Contribute to agile project planning and execution related to data engineering tasks and initiatives.\\nDesired skills:Highly preferred to have prior experience in creating DW models on SAP ECC, Salesforce systems.\\nSoft skills:to adapt to changing situations, handle multiple tasks, and meet tight deadlinesproblem solving, cross-functional analysis and forward-thinking abilities', 'job_description_neg': \"experienceCollaborate with other solution and functional teams (e.g., commercial operations, professional services, clinical education, financial administration) to find practical and ambitious solutions to these gaps and aspirations.Identify critical success metrics with which to gauge the relative performance and progress of our managed service customers over time.\\n\\nYou're the right fit if: \\nYou’ve acquired 7+ programming, data visualization, and healthcare informatics experience as well as knowledge of physiologic monitoring systems.Your skills include database design, modeling and dynamic visualization, Proficiency with R and/or Python libraries commonly used in data science, Python programming experience, hospital data flows such as CPOE, EMR, RIS, LIS and PACS. Experience in related data format standards such as HL7, DICOM, FHIR and IHE, healthcare terms and classifications (SNOMED CT, ICD10); high affinity with applying new IT platforms/dash boarding software tools for reporting and experience.You have a Master’s Degree in Computer Sciences, Biomedical Engineering, Bioinformatics, or a related field OR 10 years of work experience, preferred.You must be able to successfully perform the following minimum Physical, Cognitive and Environmental job requirements with or without accommodation for this position.You also need to have the ability to work with cross-functional teams, be self-motivated, committing to results and be flexible and quick-learning. You also should have excellent verbal and written communication skills, ability to manage complex projects along with demonstrated operational analytics and financial analysis capabilities.\\n\\nAbout Philips\\n\\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help improve the lives of others.\\n\\nLearn more about our business.Discover our rich and exciting history.Learn more about our purpose.Read more about our employee benefits.\\n\\nIf you’re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion here.\\n\\nAdditional Information\\n\\nUS work authorization is a precondition of employment. The company will not consider candidates who require sponsorship for a work-authorized visa, now or in the future.\\n\\nCompany relocation benefits will not be provided for this position. For this position, you must reside in or within commuting distance to the locations listed.\\n\\nThis requisition is expected to stay active for 45 days but may close earlier if a successful candidate is selected or business necessity dictates. Interested candidates are encouraged to apply as soon as possible to ensure consideration.\\n\\nPhilips is an Equal Employment and Opportunity Employer/Disabled/Veteran and maintains a drug-free workplace.\"}\n",
      "{'query': 'Data pipeline development, distributed data tools (Spark, Kafka), cloud data warehousing (Snowflake, BigQuery).', 'job_description_pos': 'experience.Solving problems efficiently, creatively, and completely despite constraints in time or resources.Understanding how critical it is we maintain a high bar of data security and privacy.\\n\\n\\nWe’re excited about you because you:\\n\\nHave the ability to adapt and apply evolving data technologies to business needs (which means the list of bullets below will change over time!).Have developed software using programming languages like Python, Scala, Java, Go, Ruby, etc.Have sufficient familiarity to understand SQL queries in the context of data pipelines (i.e. dbt).Have experience with distributed data tools (i.e. Spark, Flink, Kafka) on large datasets.Have worked with cloud-data warehouses (i.e. Snowflake, BigQuery, Redshift) or other warehousing solutions.Have an understanding of underlying infrastructure needed to serve production services (i.e. Kubernetes, AWS, GCP, Azure).\\n\\n\\nAbout Strava\\n\\nStrava is Swedish for “strive,” which epitomizes who we are and what we do. We’re a passionate and committed team, unified by our mission to connect athletes to what motivates them and help them find their personal best. And with billions of activity uploads from all over the world, we have a humbling and audacious vision: to be the record of the world’s athletic activities and the technology that makes every effort count.\\n\\nStrava builds software that makes the best part of our athletes’ days even better. And just as we’re deeply committed to unlocking their potential, we’re dedicated to providing a world-class, inclusive workplace where our employees can grow and thrive, too. We’re backed by Sequoia Capital, Madrone Partners and Jackson Square Ventures, and we’re expanding in order to exceed the needs of our growing community of global athletes. Our culture reflects our community – we are continuously striving to hire and engage diverse teammates from all backgrounds, experiences and perspectives because we know we are a stronger team together.\\n\\nDespite challenges in the world around us, we are continuing to grow camaraderie and positivity within our culture and we are unified in our commitment to becoming an antiracist company. We are differentiated by our truly people-first approach, our compassionate leadership, and our belief that we can bring joy and inspiration to athletes’ lives — now more than ever. All to say, it’s a great time to join Strava!\\n\\nStrava is \\n\\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\\n\\nCalifornia Consumer Protection Act Applicant Notice', 'job_description_neg': 'Qualifications3+ years of experience in analyzing and interpreting data, and managing data pipelines Proficient in data visualization through platforms like Tableau, PowerBI, or comparable tools. Proficient in Python and the development of ETL pipelines. Experience in writing intricate SQL queries. Exhibit robust oral and written communication abilities.  Preferred QualificationsExperience building applications in Python (or other scripting language) Finance and accounting reconciliation experience Logistics experience'}\n",
      "{'query': 'Data Engineering Lead, Databricks administration, Neo4j graph database expertise, ETL/ELT best practices', 'job_description_pos': 'Requirements\\n\\nExperience: At least 6 years of hands-on experience in deploying production-quality code, with a strong preference for experience in Python, Java, or Scala for data processing (Python preferred).Technical Proficiency: Advanced knowledge of data-related Python packages and a profound understanding of SQL and Databricks.Graph Database Expertise: Solid grasp of Cypher and experience with graph databases like Neo4j.ETL/ELT Knowledge: Proven track record in implementing ETL (or ELT) best practices at scale and familiarity with data pipeline tools.\\n\\nPreferred Qualifications\\n\\nProfessional experience using Python, Java, or Scala for data processing (Python preferred)\\n\\nWorking Conditions And Physical Requirements\\n\\nAbility to work for long periods at a computer/deskStandard office environment\\n\\nAbout The Organization\\n\\nFullsight is an integrated brand of our three primary affiliate companies – SAE Industry Technologies Consortia, SAE International and Performance Review Institute – and their subsidiaries. As a collective, Fullsight enables a robust resource of innovative programs, products and services for industries, their engineers and technical experts to work together on traditional and emergent complex issues that drive their future progress.\\n\\nSAE Industry Technologies Consortia® (SAE ITC) enables organizations to define and pilot best practices. SAE ITC industry stakeholders are able to work together to effectively solve common problems, achieve mutual benefit for industry, and create business value.\\n\\nThe Performance Review Institute® (PRI) is the world leader in facilitating collaborative supply chain oversight programs, quality management systems approvals, and professional development in industries where safety and quality are shared values.\\n\\nSAE International® (SAEI) is a global organization serving the mobility sector, predominantly in the aerospace, automotive and commercial-vehicle industries, fostering innovation, and enabling engineering professionals. Since 1905, SAE has harnessed the collective wisdom of engineers around the world to create industry-enabling standards. Likewise, SAE members have advanced their knowledge and understanding of mobility engineering through our information resources, professional development, and networking.', 'job_description_neg': \"Qualifications:Master's or Ph.D. in Computer Science, Statistics, Mathematics, or a related field.Minimum of 5 years of experience in a data science role, with a focus on payments fraud detection and prevention.Proficiency in programming languages such as Python, R, or Scala, and experience with data manipulation and analysis libraries (e.g., pandas, NumPy).Strong understanding of machine learning techniques, including supervised and unsupervised learning algorithms.Experience with big data technologies such as Hadoop, Spark, or Hive.Excellent problem-solving skills and the ability to translate business requirements into technical solutions.Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.Prior experience working in the financial technology industry is a plus.\\nBenefits:Opportunity to work with some incredibly successful leaders in the FinTech space.Equity at a Unicorn company.Fully remote.Full health & dental coverage.\\nDoes this sound like it might be a good fit for you? Apply below today and we can set up some time to speak.\"}\n",
      "{'query': 'Big Data Engineer, Spark/Scala, Hive, NoSQL databases', 'job_description_pos': 'Skills : Big Data Engineer with 3-4 years of hands-on Spark/Scala and Hive experience. The candidate must be able to work a hybrid schedule with the team in Phoenix, AZ (onsite every Tues/Wed/Thurs). If the candidate is not located in Phoenix, but wiling to relocate and work onsite, they can begin working remotely and then relocate within the first few months while on contract. Any experience with the following items are pluses: Experience with NoSQL databases (MapR DB, HBase, Cassandra)Experience with Big Data Components/Frameworks such as Hadoop (MapR), Spark, YarnExperience with Big Data querying tools such HiveExperience in Big Query and Cloud computing. Thanks & Regards\\n Sonam NakotiSenior Recruitment Specialist Office: (470) 900-1049 E-Mail: sonamn@codeforce.com LinkedIn: https://www.linkedin.com/in/naksona/', 'job_description_neg': \"Experience in crunching data? Love working with data and providing business insights? Power BI Rockstar? We'd love to talk to you! \\n\\nResponsibilities\\n\\n Work closely with Business stakeholders to access reporting requirements and confirm existing reporting capabilities  Develop reporting and analytics to identify opportunities for process improvement; provide expert-level advice on the implementation of operational process; continual refinement of analytics to drive operational excellence  Develop quality assurance process relating to business intelligence reporting; conduct reviews of output; consult with end users, implement resolution to any deficiencies  Develop and implement reporting audits to ensure accuracy and compliance  Collaborate with cross-functional teams and senior stakeholders to identify and understand key business challenges, translating them into data-driven insights and actionable recommendations  Create compelling visualizations and interactive dashboards to effectively communicate analytical findings to non-technical stakeholders, present insights, and recommendations to senior leadership in a clear and concise manner  Develop data models and frameworks to organize and structure data effectively and create visually appealing and informative reports, dashboards, and presentations  Present complex data in a simplified and understandable format for non-technical stakeholders and utilize data visualization tools like Power BI to enhance data storytelling  Manage data Gathering, Analyzing, Cleaning, transforming, and manipulating various sorts of data using SQL, Microsoft Excel (Pivot tables, VLOOK UP, etc.) and Power BI to ensure data accuracy and consistency  Provide prompt, effective day-to-day support for stakeholders on data, dashboarding, tooling, and reporting  Accountable for efficient transition and delivery of scheduled and support ad-hoc reports and analysis requests \\n\\nQualifications\\n\\n Must possess one or more of the following:  Associate degree in Business Administration, Management, or related field with a minimum of three (3) years of management experience in the financial and team leadership aspects of a large semi-independent business enterprise  High school diploma or equivalent with a minimum of five (5) years of management experience in the financial and team leadership aspects of a large semi-independent business enterprise  Experience with data visualization tools such as Power BI  Exceptional oral, written, and presentation skills  Ability to work effectively both independently and as part of a team  Knowledge of file management and other administrative procedures  Ability to work on tight deadlines  Must possess strong oral, written, and analytical skills to effectively convey complex concepts and findings to both technical and non-technical stakeholders  Effective oral and written communication  Planning and organizing  Proficiency with Microsoft Office Applications  Problem solving  Analyzing, predicting  Active listening  Write informatively, clearly, and accurately  Identify critical issues quickly and accurately  Teamwork  Attention to detail \\nPreferred Qualifications\\n\\n Working knowledge of Finance-related processes in ERP environment, PeopleSoft, WinTeam  Intermediate skill level in Microsoft Office; Excel in particular  Experience working with internal and external clients \\n\\nBenefits\\n\\n Medical, dental, vision, basic life, AD&D, and disability insurance  Enrollment in our company’s 401(k)plan, subject to eligibility requirements  Eight paid holidays annually, five sick days, and four personal days  Vacation time offered at an accrual rate of 3.08 hours biweekly. Unused vacation is only paid out where required by law. \\n\\n Closing \\n\\nAllied Universal® is \\n\\nIf you have any questions regarding \\n\\n Requisition ID \\n\\n2024-1200911\"}\n",
      "{'query': 'IBOR data integrity, duplicate resolution, Initiate TM Inspector Tool', 'job_description_pos': 'Qualifications\\n• Excellent decision making abilities and effective problem solving skills. Ability to analyze data and make decisions based on the information gathered.• Analytical experience (e.g. data and process analysis, quality metrics, policies, standards, and processes) preferred.• Strong time management skills; organized with strong focus and excellent attention to detail.• Strong verbal and written communication skills.• Experience with Customer data analysis a plus', 'job_description_neg': \"experience working with very large amounts of streaming data. Working on a brand new R&D project, they are looking for someone with innovative ideas and technical vision, utilizing their 10+ years of industry experience.\\n\\nIn this role you will be using Snowflake, Python, Spark, PySpark, SQL, AWS, Airflow, Redshift, and Databricks. If you have experience building systems from scratch, working at startups, and/or being a technical leader in the Data space, always adopting new technologies than this is the space for you. This company is using AI to improve the world and customer experiences. If you're interested in learning more, apply today!\\n\\nRequired Skills & Experience\\n\\n10+ years professional Data Engineering Experience Experience being a technical thought leader Experience working with large amounts of streaming data Python, Cloud, Snowflake, Airflow, and Spark professional Interest in the AI/Computer Vision space \\n\\nDesired Skills & Experience\\n\\nBachelors in STEM field Excellent written and verbal communication skills \\n\\nThe Offer\\n\\nYou Will Receive The Following Benefits\\n\\nMedical Insurance Dental Benefits Vision Benefits Paid Sick Time Paid Time Off 401(k) with match Equity Bonus Hybrid flexibility (2-3 days onsite in LA) \\n\\nApplicants must be currently authorized to work in the US on a full-time basis now and in the future.\\n\\nPosted By: Cassi Benson\"}\n",
      "{'query': 'Microsoft Synapse, Azure DevOps, Microservice/API Development', 'job_description_pos': 'Skills/Domain: Microsoft Azure, Synapse, Spark, Python, Angular, C#, .NET, DevOps, Azure Function,Microservice/API Development, Power BIRoles and', 'job_description_neg': \"Requirements NOTE: Applicants with an Active TS Clearance preferred Requirements * High School diploma or GED, Undergraduate degree preferred Ability to grasp and understand the organization and functions of the customer Meticulous data entry skills Excellent communication skills; oral and written Competence to review, interpret, and evaluate complex legal and non-legal documents Attention to detail and the ability to read and follow directions is extremely important Strong organizational and prioritization skills Experience with the Microsoft Office suite of applications (Excel, PowerPoint, Word) and other common software applications, to include databases, intermediate skills preferred Proven commitment and competence to provide excellent customer service; positive and flexible Ability to work in a team environment and maintain a professional dispositionThis position requires U.S. Citizenship and a 7 (or 10) year minimum background investigation ** NOTE: The 20% pay differential is dependent upon the customer's order for services and requires an Active Top-Secret security clearance. Agency Overview The mission of the Federal Bureau of Investigation (FBI) is to protect the American people and uphold the Constitution of the United States. FBI investigates a broad range of criminal violations, integrating the use of asset forfeiture into its overall strategy to eliminate targeted criminal enterprises. The FBI has successfully used asset forfeiture in White Collar Crime, Organized Crime, Drug, Violent Crime and Terrorism investigations. Benefits Overview At EnProVera, we recognize the diverse needs of our employees and strive to provide an excellent package to help meet those needs. Comprehensive benefits are offered with greater choice and flexibility to support your health, work-life balance, and professional growth. A package providing employee only coverage can be built around our basic plans at $0 employee cost for: Medical, Dental, Vision, Term Life Insurance, Accidental Death -amp; Dismemberment Insurance, Short-Term Disability, and Employee Assistance Program.\"}\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from huggingface\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Data: https://huggingface.co/datasets/shawhin/ai-job-embedding-finetuning\n",
    "dataset = load_dataset(\"shawhin/ai-job-embedding-finetuning\")\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(dataset) # structure\n",
    "print(\"*\"*50)\n",
    "# print(dataset['train'][0]) # example\n",
    "[print(dataset['train'][i]) for i in range(10)]\n",
    "print(\"*\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76b980",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf7403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47113b38",
   "metadata": {},
   "source": [
    "---\n",
    "Referencies:\n",
    "- [Fine-Tuning Text Embeddings For Domain-specific Search (w/ Python)](https://www.youtube.com/watch?v=hOLBrIjRAj4 \"Fine-Tuning Text Embeddings For Domain-specific Search (w/ Python)\")\n",
    "- [Sentence transformer - Trainer](https://sbert.net/docs/sentence_transformer/training_overview.html#trainer \"Sentence transformer - Trainer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
